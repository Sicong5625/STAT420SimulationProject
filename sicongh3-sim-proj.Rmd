---
title: 'Week 6 - Simulation Project'
author: "STAT 420, Summer 2020, Sicong He"
date: ''
output:
  html_document: 
    theme: readable
    toc: yes  
  pdf_document: default
urlcolor: cyan
---

***

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80)
library(knitr)
opts_chunk$set(cache = TRUE, autodep = TRUE)
```


```{r}
birthday = 18760613
set.seed(birthday)
```

# Simulation Study 1: Significance of Regression
## Introduction
This simulation study is aimed to investigate the significance of regression test. Doing so can help us to have a deeper understanding of the significance of regression test and its limitations as well as the applied scenarios comparing with other criteria like $R^2$ when we discuss the relationship of data.   

To investigate the test, we will use the same data to simulate two different models: the "significant model" and the "non-significant" model. We will use the data we get friom the simulation to run the test and discuss the accuracey of the result.  

## Method 
we will use the `study_1.csv` data set as our variable to do simulation. We will generate two models, one is significant and the other is not.
1. The **"significant"** model

\[
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \epsilon_i
\]

where $\epsilon_i \sim N(0, \sigma^2)$ and

- $\beta_0 = 3$,
- $\beta_1 = 1$,
- $\beta_2 = 1$,
- $\beta_3 = 1$.


2. The **"non-significant"** model

\[
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \epsilon_i
\]

where $\epsilon_i \sim N(0, \sigma^2)$ and

- $\beta_0 = 3$,
- $\beta_1 = 0$,
- $\beta_2 = 0$,
- $\beta_3 = 0$.

For both, we will consider a sample size of $25$ and three possible levels of noise. That is, three values of $\sigma$.

- $n = 25$
- $\sigma \in (1, 5, 10)$  

import data:

```{r}
study_1 <- read.csv("study_1.csv")
n = 25
x1 = study_1$x1
x2 = study_1$x2
x3 = study_1$x3

#import parameters
beta0_sig = 3
beta1_sig = 1
beta2_sig = 1
beta3_sig = 1
beta0_non = 3
beta1_non = 0
beta2_non = 0
beta3_non = 0

```
```{r}
#models
significant = beta0_sig + beta1_sig*x1+beta2_sig*x2+beta3_sig*x3
nonsignificant = beta0_non + beta1_non*x1 + beta2_non*x2 + beta3_non*x3
```

simulation for sigma = 1
```{r}
stat1 = data.frame(fstat = rep(0, 2000), pvalue = rep(0,2000), sigrsquared = rep(0, 2000), nonrsquared = rep(0, 2000))
for (variable in 1:2000) {
  ep1 = rnorm(n, 0, 1)
  sig1 = significant +ep1
  
  study_1$y = sig1
  
  non1 = nonsignificant + ep1
  siglm1 = lm(y ~ x1+x2+x3, data = study_1)
  nonsiglm1 =  lm(non1 ~ x1+x2+x3)
  
  stat1$sigrsquared[variable] = summary(siglm1)$r.squared
  stat1$fstat[variable] = summary(siglm1)$fstatistic[[1]]
  stat1$pvalue[variable] = pf(stat1$fstat[variable], 3, 21, lower.tail = FALSE)
  stat1$nonrsquared[variable] = summary(nonsiglm1)$r.squared
}



```
simulation for sigma = 5

```{r}
stat2 = data.frame(fstat = rep(0, 2000), pvalue = rep(0,2000), sigrsquared = rep(0, 2000), nonrsquared = rep(0, 2000))
for (variable in 1:2000) {
  ep2 = rnorm(n, 0, 5)
  sig2 = significant + ep2
  non2 = nonsignificant + ep2
  study_1$y = sig2
  
  siglm2 = lm(y~x1+x2+x3,data = study_1)
  nonsiglm2 =  lm(non2~x1+x2+x3)
  
  stat2$sigrsquared[variable] = summary(siglm2)$r.squared
  stat2$fstat[variable] = summary(siglm2)$fstatistic[[1]]
  stat2$pvalue[variable] = pf(stat2$fstat[variable], 3, 21, lower.tail = FALSE)
  stat2$nonrsquared[variable] = summary(nonsiglm2)$r.squared
}
```

simulation for sigma = 10
```{r}

stat3 = data.frame(fstat = rep(0, 2000), pvalue = rep(0,2000), sigrsquared = rep(0, 2000))
for (variable in 1:2000) {
  ep3 = rnorm(n, 0, 10)
  sig3 = significant + ep3
  non3 = nonsignificant + ep3
  study_1$y = sig3
  
  siglm3 = lm(y~x1+x2+x3,data = study_1)
  nonsiglm3 =  lm(non3~x1+x2+x3)
  
  stat3$sigrsquared[variable] = summary(siglm3)$r.squared
  stat3$fstat[variable] = summary(siglm3)$fstatistic[[1]]
  stat3$pvalue[variable] =pf(stat3$fstat[variable], 3, 21, lower.tail = FALSE)
  stat3$nonrsquared[variable] = summary(nonsiglm3)$r.squared
}

```
  
  
## Result

For each model and $\sigma$ combination, I used $2000$ simulations for each sigma value, which means a total of 12000 models. I recorded the f-statistic. p-value and $R^2$ for all significant model.  

Here are the graphs I drew to compare the impact of the change in sigma(error term) to the entire model. We know that the F statistics should follow a f distribution, and to me it seems the p-value follows a sort of gamma distribution. The redline in the p-value plots is a gamma distribution, and the redline in the f statistics plot is the f curve with 3, 21 degrees of freedom.
```{r}
par(mfrow=c(2,3))
plota = hist(stat1$pvalue*100, breaks = 50,col = "lightblue", prob = TRUE, main = "p-value for sigma = 1", xlab = "p-values in percent", 
             xlim = c(min(stat1$pvalue*100), max(stat1$pvalue*100)))
curve(dgamma(x,0.5), add = TRUE, lwd = 3, col ="brown2")
plotb = hist(stat2$pvalue*100, breaks = 50,col = "lightblue",prob = TRUE,  main = "p-value for sigma = 5", xlab = "p-values in percent")
curve(dgamma(x,0.5), add = TRUE, lwd = 3, col ="brown2")

plotc = hist(stat3$pvalue*100, breaks = 50,col = "lightblue",prob = TRUE,  main = "p-value for sigma = 10", xlab = "p-values in percent")
curve(dgamma(x,0.5), add = TRUE, lwd = 3, col ="brown2")

plotd = hist(stat1$fstat, breaks = 50,col = "lightblue",prob = TRUE,
             main = "f-statisitcs for sigma = 1", xlab = "f-statistic")
curve(df(x,3, 21), add = TRUE, lwd = 3, col ="brown2")

plote = hist(stat2$fstat, breaks = 50,col = "lightblue",prob = TRUE,  
             main = "f-statisitc sigma = 5", xlab = "f-statistic")
curve(df(x,3, 21), add = TRUE, lwd = 3, col ="brown2")

plotf = hist(stat3$fstat, breaks = 50,col = "lightblue",prob = TRUE,  
             main = "f-statisitc for sigma = 10", xlab = "f-statistic")
curve(df(x,3, 21), add = TRUE, lwd = 3, col ="brown2")
```

We can conclude from the plots that when sigma is small, we get a precise estimation of the significance. The increase of the level of noise strongly affects the result. The larger the sigma is, the more right skewed f statistic is. Thus when sigma is large enough, we could expect the f statistic we get fits the f curve with 3 and 21 degrees of freedom. The p-value decreases as the f statistic increases.

```{r}
par(mfrow = c(2,3))
plotf = hist(stat1$sigrsquared, breaks = 25, main = "R^2 significant, sigma = 1", xlab = "R^2 value")
plotg = hist(stat2$sigrsquared, breaks = 25, main = "R^2 significan, sigma = 5", xlab = "R^2 value")
ploth = hist(stat3$sigrsquared, breaks = 25, main = "R^2 significant, sigma = 10", xlab = "R^2 value")
ploti = hist(stat1$nonrsquared, breaks = 25, main = "R^2 nonsignificant, sigma = 1", xlab = "R^2 value")
plotj = hist(stat2$nonrsquared, breaks = 25, main = "R^2 nonsignificant, sigma = 5", xlab = "R^2 value")
plotk = hist(stat3$nonrsquared, breaks = 25, main = "R^2 nonsignificant, sigma = 10", xlab = "R^2 value")
```

 we can see from the $R^2$ plots that when $\sigma$ is small, there is a significant difference between the $R^2$ of significant model and nonsignificant model. However, if $\sigma$ increase, there will be a significant reduce in $R^2$ of the significant model. The $R^2$ of the two models are almost the same when $\sigma = 10$. $R^2$ is less sensitive than p-values from the f statistic.$R^2$ shows similar trend as f statistic. They all become right skewed when $\sigma$ increased.

## Discussion

From the result we get, we can concluded that significance of regression test do have its limitations: it could be largely affected by the change of variance. However, it performs way better than $R^2$ when $\sigma$ is relatively small compared to the sample size. And this is why we tend to use significance of regression test rather than $R^2$ in most scenarios: most data and samples we got from our daily life can easily satisfy this premise.

# Simulation Study 2: Using RMSE for Selection?

## Introduction
We often use RMSE to decide the "best" model. In this project I would like to explore how RMSE selection works. I expect RMSE to select the most appropriate model. To explore RMSE, we will use data from `study_2.csv` and see how RMSE perform under models with different numbers of predictors.  

## Method

We will simulate from the model

\[
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \beta_4 x_{i4} + \beta_5 x_{i5} + \beta_6 x_{i6} + \epsilon_i
\]

where $\epsilon_i \sim N(0, \sigma^2)$ and

- $\beta_0 = 0$,
- $\beta_1 = 3$,
- $\beta_2 = -4$,
- $\beta_3 = 1.6$,
- $\beta_4 = -1.1$,
- $\beta_5 = 0.7$,
- $\beta_6 = 0.5$.

We will consider a sample size of $500$ and three possible levels of noise. That is, three values of $\sigma$.

- $n = 500$
- $\sigma \in (1, 2, 4)$


I will Use the data found in [`study_2.csv`](study_2.csv) for the values of the predictors.  
Inport data:
```{r}
study_2 <- read.csv("study_2.csv")
n = 500
x1 = study_2$x1
x2 = study_2$x2
x3 = study_2$x3
x4 = study_2$x4
x5 = study_2$x5
x6 = study_2$x6
#import parameters
beta0 = 0
beta1 = 3
beta2 = -4
beta3 = 1.6
beta4 = -1.1
beta5 = 0.7
beta6 = 0.5
```

basic model
```{r}
lm = beta0 + beta1*x1 + beta2*x2 + beta3*x3 + beta4*x4 + beta5*x5 + beta6*x6
```
Helper functions to calculate RMSE and to generate simulation.
```{r}
rmse = function(actural, predicted) {
  a = (actural-predicted)^2
  result = sqrt(mean(a))
  return(result)
}
simulation = function(y, data = study_2) {
  study_2$y = y
  index = sample(1:500, 250)
  trainx = data[index,]
  testx = data[-index,]
  model1 = lm(y~x1, data = trainx)
  model2 = lm(y~x1 +x2, data = trainx)
  model3 = lm(y~x1 +x2 + x3, data = trainx)
  model4 = lm(y~x1 +x2 + x3 + x4, data = trainx)
  model5 = lm(y~x1 +x2 + x3 + x4 +x5, data = trainx)
  model6 = lm(y~x1 +x2 + x3 + x4 +x5 + x6, data = trainx)
  model7 = lm(y~x1 +x2 + x3 + x4 +x5 + x6 + x7, data = trainx)
  model8 = lm(y~x1 +x2 + x3 + x4 +x5 + x6 + x7 + x8, data = trainx)
  model9 = lm(y~., data = trainx)
  
  y_fitted1 = model1$fitted.values
  y_fitted2 = model2$fitted.values
  y_fitted3 = model3$fitted.values
  y_fitted4 = model4$fitted.values
  y_fitted5 = model5$fitted.values
  y_fitted6 = model6$fitted.values
  y_fitted7 = model7$fitted.values
  y_fitted8 = model8$fitted.values
  y_fitted9 = model9$fitted.values
  y_fitted = cbind(y_fitted1, y_fitted2 ,y_fitted3, y_fitted4, y_fitted5, y_fitted6, y_fitted7, y_fitted8, y_fitted9)

  tra_RMSE = rep(0,9)
  for (variable in 1:9) {
    tra_RMSE[variable] = rmse(y,y_fitted[variable])
  }
  
  y_hat1 = predict(model1, newdata = testx[,2:10])
  y_hat2 = predict(model2, newdata = testx[,2:10])
  y_hat3 = predict(model3, newdata = testx[, 2:10])
  y_hat4 = predict(model4, newdata = testx[, 2:10])
  y_hat5 = predict(model5, newdata = testx[, 2:10])
  y_hat6 = predict(model6, newdata = testx[, 2:10])
  y_hat7 = predict(model7, newdata = testx[, 2:10])
  y_hat8 = predict(model8, newdata = testx[, 2:10])
  y_hat9 = predict(model9, newdata = testx[, 2:10])
  y_hat = cbind(y_hat1, y_hat2, y_hat3, y_hat4, y_hat5, y_hat6, y_hat7, y_hat8, y_hat9)
  tst_RMSE = rep(0,9)
  for (variable in 1:9) {
    tst_RMSE[variable] = rmse(testx[,1],y_hat[variable])
  }
  result = cbind(tra_RMSE, tst_RMSE)
  return(result)
}

```
After each simulation, I will randomly split the data into train and test sets of equal sizes (250 observations for training, 250 observations for testing).

And for each, I'll fit **nine** models, with forms:

- `y ~ x1`
- `y ~ x1 + x2`
- `y ~ x1 + x2 + x3`
- `y ~ x1 + x2 + x3 + x4`
- `y ~ x1 + x2 + x3 + x4 + x5`
- `y ~ x1 + x2 + x3 + x4 + x5 + x6`, 
- `y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7`
- `y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8`
- `y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9`

For each model, I calculate Train and Test RMSE.

\[
\text{RMSE}(\text{model, data}) = \sqrt{\frac{1}{n} \sum_{i = 1}^{n}(y_i - \hat{y}_i)^2}
\]

And I repeat the process 1000 times for each value of sigma:

sigma = 1
```{r}
train_RMSE1 = data.frame(m1 = rep(0, 1000), m2 = rep(0, 1000), m3= rep(0, 1000), m4= rep(0, 1000), m5= rep(0, 1000),
                        m6= rep(0, 1000), m7= rep(0, 1000), m8= rep(0, 1000), m9= rep(0, 1000))
test_RMSE1 = data.frame(m1 = rep(0, 1000), m2 = rep(0, 1000), m3= rep(0, 1000), m4= rep(0, 1000), m5= rep(0, 1000),
                        m6= rep(0, 1000), m7= rep(0, 1000), m8= rep(0, 1000), m9= rep(0, 1000))
for (variable in 1:1000) {
  #sigma = 1
  ep1 = rnorm(n, 0, 1)
  sim1 = lm + ep1
  rmse1 = simulation(sim1)
  train_RMSE1$m1[variable] = rmse1[1,1]
  train_RMSE1$m2[variable] = rmse1[2,1]
  train_RMSE1$m3[variable] = rmse1[3,1]
  train_RMSE1$m4[variable] = rmse1[4,1]
  train_RMSE1$m5[variable] = rmse1[5,1]
  train_RMSE1$m6[variable] = rmse1[6,1]
  train_RMSE1$m7[variable] = rmse1[7,1]
  train_RMSE1$m8[variable] = rmse1[8,1]
  train_RMSE1$m9[variable] = rmse1[9,1]
  
  test_RMSE1$m1[variable] = rmse1[1,2]
  test_RMSE1$m2[variable] = rmse1[2,2]
  test_RMSE1$m3[variable] = rmse1[3,2]
  test_RMSE1$m4[variable] = rmse1[4,2]
  test_RMSE1$m5[variable] = rmse1[5,2]
  test_RMSE1$m6[variable] = rmse1[6,2]
  test_RMSE1$m7[variable] = rmse1[7,2]
  test_RMSE1$m8[variable] = rmse1[8,2]
  test_RMSE1$m9[variable] = rmse1[9,2]
  }
```

sigma = 2
```{r}
train_RMSE2 = data.frame(m1 = rep(0, 1000), m2 = rep(0, 1000), m3= rep(0, 1000), m4= rep(0, 1000), m5= rep(0, 1000),
                        m6= rep(0, 1000), m7= rep(0, 1000), m8= rep(0, 1000), m9= rep(0, 1000))
test_RMSE2 = data.frame(m1 = rep(0, 1000), m2 = rep(0, 1000), m3= rep(0, 1000), m4= rep(0, 1000), m5= rep(0, 1000),
                        m6= rep(0, 1000), m7= rep(0, 1000), m8= rep(0, 1000), m9= rep(0, 1000))
for (variable in 1:1000) {
  #sigma = 2
  ep2 = rnorm(n, 0, 2)
  sim2 = lm + ep2
  rmse2 = simulation(sim2)
  train_RMSE2$m1[variable] = rmse2[1,1]
  train_RMSE2$m2[variable] = rmse2[2,1]
  train_RMSE2$m3[variable] = rmse2[3,1]
  train_RMSE2$m4[variable] = rmse2[4,1]
  train_RMSE2$m5[variable] = rmse2[5,1]
  train_RMSE2$m6[variable] = rmse2[6,1]
  train_RMSE2$m7[variable] = rmse2[7,1]
  train_RMSE2$m8[variable] = rmse2[8,1]
  train_RMSE2$m9[variable] = rmse2[9,1]
  
  test_RMSE2$m1[variable] = rmse2[1,2]
  test_RMSE2$m2[variable] = rmse2[2,2]
  test_RMSE2$m3[variable] = rmse2[3,2]
  test_RMSE2$m4[variable] = rmse2[4,2]
  test_RMSE2$m5[variable] = rmse2[5,2]
  test_RMSE2$m6[variable] = rmse2[6,2]
  test_RMSE2$m7[variable] = rmse2[7,2]
  test_RMSE2$m8[variable] = rmse2[8,2]
  test_RMSE2$m9[variable] = rmse2[9,2]
  }
```

sigma = 4
```{r}
train_RMSE3 = data.frame(m1 = rep(0, 1000), m2 = rep(0, 1000), m3= rep(0, 1000), m4= rep(0, 1000), m5= rep(0, 1000),
                        m6= rep(0, 1000), m7= rep(0, 1000), m8= rep(0, 1000), m9= rep(0, 1000))
test_RMSE3 = data.frame(m1 = rep(0, 1000), m2 = rep(0, 1000), m3= rep(0, 1000), m4= rep(0, 1000), m5= rep(0, 1000),
                        m6= rep(0, 1000), m7= rep(0, 1000), m8= rep(0, 1000), m9= rep(0, 1000))
for (variable in 1:1000) {
  #sigma = 4
  ep3 = rnorm(n, 0, 4)
  sim3 = lm + ep3
  rmse3 = simulation(sim3)

  train_RMSE3$m1[variable] = rmse3[1,1]
  train_RMSE3$m2[variable] = rmse3[2,1]
  train_RMSE3$m3[variable] = rmse3[3,1]
  train_RMSE3$m4[variable] = rmse3[4,1]
  train_RMSE3$m5[variable] = rmse3[5,1]
  train_RMSE3$m6[variable] = rmse3[6,1]
  train_RMSE3$m7[variable] = rmse3[7,1]
  train_RMSE3$m8[variable] = rmse3[8,1]
  train_RMSE3$m9[variable] = rmse3[9,1]
  
  test_RMSE3$m1[variable] = rmse3[1,2]
  test_RMSE3$m2[variable] = rmse3[2,2]
  test_RMSE3$m3[variable] = rmse3[3,2]
  test_RMSE3$m4[variable] = rmse3[4,2]
  test_RMSE3$m5[variable] = rmse3[5,2]
  test_RMSE3$m6[variable] = rmse3[6,2]
  test_RMSE3$m7[variable] = rmse3[7,2]
  test_RMSE3$m8[variable] = rmse3[8,2]
  test_RMSE3$m9[variable] = rmse3[9,2]
  }


```

## Result

To observe the difference in RMSE by 3 sigmas, I draw the following plot:
```{r}

par(mfrow = c(2,3))
plot21 = boxplot(train_RMSE1, main = "test RMSE sigma = 1")
plot22 = boxplot(train_RMSE2,  main = "test RMSE sigma = 2")
plot23 = boxplot(train_RMSE3, main = "test RMSE sigma = 4")
plot24 = boxplot(test_RMSE1, main = "train RMSEsigma = 1")
plot25 = boxplot(test_RMSE2, main = "train RMSE sigma = 2")
plot26 = boxplot(test_RMSE3, main = "train RMSE sigma = 4")

```
    
    From the box plot, we could observed that the more variable we add into the model, the lower RMSE we get. The first few variables added to the model decresed the RMSE significantly, as shown in graph, for all three sigma values, the decrease of RMSE for model 1-4 is dramatic, however, as we reached model8 and 9, we cannot really observe the difference between their RMSE.   
 
The true best model according to the data is:
-sigma = 1
```{r}
colMeans(test_RMSE1)
min(colMeans(test_RMSE1))
colMeans(train_RMSE1)
min(colMeans(train_RMSE1))
```
The best model should be model 9, but our RMSE tells us that the best model is model 1. However, since there is only a 0.1  difference between the two models, I would consider it to be correct.

-sigma = 2
```{r}
colMeans(test_RMSE2)
min(colMeans(test_RMSE2))
colMeans(train_RMSE2)
min(colMeans(train_RMSE2))
```
The best model should be model 9, but our RMSE tells us that the best model is model 5. However, since there is only a less than 0.1  difference between the two models, I think it's ok to state that the result is correct. And also, model 5 is simpler than model 9.  

-sigma = 4
```{r}
colMeans(test_RMSE3)
min(colMeans(test_RMSE3))
colMeans(train_RMSE3)
min(colMeans(train_RMSE3))
```
The best model should be model 9, but our RMSE tells us that the best model is model 3. This time the difference is about 0.3, which is relatively lack of accuracy, this time I would consider it not being the best answer.


## Discussion

We often use the mean of RMSE to describe the models as well as choosing from various models. But this time the data tells us that though for most of the time we can rely on RMSE, but for a sigma relatively large compared to the data, or in other words, we know there is a high level of noise, we should be cautious while using RMSE. Make sure to observe more data before making the conclusion and also observe more characteristics of the RMSE we get. As we can see, if we use median as a criteria, RMSE is usually precise. 



